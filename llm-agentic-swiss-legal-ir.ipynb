{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":128290,"databundleVersionId":15567168,"sourceType":"competition"}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rahulchauhan016/llm-agentic-swiss-legal-ir?scriptVersionId=296691177\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# LLM Agentic Swiss Legal IR\n\n\n\n---\n\n### Overview\nThis notebook implements a complete 6-stage legal citation retrieval pipeline:\n\n1. **BM25 Sparse Search** - Fast keyword matching (2000 candidates)\n2. **Dense Retrieval** - Multilingual E5-Large embeddings (500 candidates)\n3. **RRF Fusion** - Combine rankings (300 merged candidates)\n4. **Deduplication** - Consolidate to unique documents\n5. **Cross-Encoder Reranking** - Precise ranking (top 20)\n6. **Citation Extraction** - Return top K results\n\n","metadata":{}},{"cell_type":"markdown","source":"## Cell 1: Install Dependencies\n\nInstall required packages for the pipeline.","metadata":{}},{"cell_type":"code","source":"!pip install -q sentence-transformers rank-bm25 faiss-cpu tqdm pandas numpy\nprint(\"âœ… Dependencies installed successfully\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:07:14.071404Z","iopub.execute_input":"2026-02-09T08:07:14.071654Z","iopub.status.idle":"2026-02-09T08:07:19.754298Z","shell.execute_reply.started":"2026-02-09T08:07:14.071632Z","shell.execute_reply":"2026-02-09T08:07:19.753432Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hâœ… Dependencies installed successfully\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Cell 2: Import Libraries & Configure GPU\n\nSet up all necessary imports and GPU configuration.","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport gc\nimport numpy as np\nimport pandas as pd\nimport faiss\nimport torch\n\nfrom tqdm import tqdm\nfrom sentence_transformers import SentenceTransformer\nfrom rank_bm25 import BM25Okapi\nfrom collections import defaultdict\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:08:05.182468Z","iopub.execute_input":"2026-02-09T08:08:05.183275Z","iopub.status.idle":"2026-02-09T08:08:33.574887Z","shell.execute_reply.started":"2026-02-09T08:08:05.183237Z","shell.execute_reply":"2026-02-09T08:08:33.574298Z"}},"outputs":[{"name":"stderr","text":"2026-02-09 08:08:17.734099: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770624497.893412      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770624497.940331      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770624498.318596      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770624498.318630      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770624498.318633      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770624498.318635      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Cell 3: Configuration \n\n","metadata":{}},{"cell_type":"code","source":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nCHUNK_SIZE = 800\nCHUNK_OVERLAP = 100\nEMBED_BATCH = 64\n\nEMB_CACHE = \"/kaggle/working/chunks_emb.npy\"\nIDX_CACHE = \"/kaggle/working/chunks.index\"\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:08:43.407731Z","iopub.execute_input":"2026-02-09T08:08:43.408807Z","iopub.status.idle":"2026-02-09T08:08:43.412586Z","shell.execute_reply.started":"2026-02-09T08:08:43.408774Z","shell.execute_reply":"2026-02-09T08:08:43.411823Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# 3.1- Cleaning Function","metadata":{}},{"cell_type":"code","source":"def clean(text):\n    text = str(text).lower()\n    text = re.sub(r\"[^a-z0-9Ã¤Ã¶Ã¼ÃŸ\\s]\", \" \", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:08:47.469633Z","iopub.execute_input":"2026-02-09T08:08:47.470184Z","iopub.status.idle":"2026-02-09T08:08:47.473942Z","shell.execute_reply.started":"2026-02-09T08:08:47.470153Z","shell.execute_reply":"2026-02-09T08:08:47.473406Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import os\n\nfor root, _, files in os.walk(\"/kaggle/input\"):\n    for f in files:\n        print(os.path.join(root, f))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:08:53.090304Z","iopub.execute_input":"2026-02-09T08:08:53.090621Z","iopub.status.idle":"2026-02-09T08:08:53.097818Z","shell.execute_reply.started":"2026-02-09T08:08:53.090594Z","shell.execute_reply":"2026-02-09T08:08:53.097216Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/llm-agentic-legal-information-retrieval/sample_submission.csv\n/kaggle/input/llm-agentic-legal-information-retrieval/laws_de.csv\n/kaggle/input/llm-agentic-legal-information-retrieval/val.csv\n/kaggle/input/llm-agentic-legal-information-retrieval/court_considerations.csv\n/kaggle/input/llm-agentic-legal-information-retrieval/train.csv\n/kaggle/input/llm-agentic-legal-information-retrieval/test.csv\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Cell 4: Load Corpus","metadata":{}},{"cell_type":"code","source":"print(\"Loading legal corpus...\\n\")\n\nbase = \"/kaggle/input/llm-agentic-legal-information-retrieval\"\n\nlaws_path = os.path.join(base, \"laws_de.csv\")\ncourts_path = os.path.join(base, \"court_considerations.csv\")\n\nlaws = pd.read_csv(laws_path)\ncourts = pd.read_csv(courts_path)\n\nprint(\"Laws:\", laws.shape)\nprint(\"Court decisions:\", courts.shape)\n\ncorpus = pd.concat([\n    laws[[\"text\", \"citation\"]],\n    courts[[\"text\", \"citation\"]]\n], ignore_index=True)\n\ncorpus[\"text\"] = corpus[\"text\"].fillna(\"\")\ncorpus[\"clean\"] = corpus[\"text\"].apply(clean)\n\nprint(\"Corpus ready:\", len(corpus))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:13:16.657444Z","iopub.execute_input":"2026-02-09T08:13:16.658047Z","iopub.status.idle":"2026-02-09T08:17:04.436671Z","shell.execute_reply.started":"2026-02-09T08:13:16.658019Z","shell.execute_reply":"2026-02-09T08:17:04.436035Z"}},"outputs":[{"name":"stdout","text":"Loading legal corpus...\n\nLaws: (175933, 3)\nCourt decisions: (2476315, 2)\nCorpus ready: 2652248\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Cell 5: Preprocess Data\n\nClean text and create chunks.","metadata":{}},{"cell_type":"code","source":"print(\"Reducing corpus size safely...\\n\")\n\n# -----------------------------\n# 1. Remove exact duplicates\n# -----------------------------\nbefore = len(corpus)\n\ncorpus[\"text\"] = corpus[\"text\"].fillna(\"\")\ncorpus = corpus.drop_duplicates(subset=[\"text\"]).reset_index(drop=True)\n\nafter = len(corpus)\nprint(f\"Removed duplicate docs: {before - after:,}\")\nprint(f\"Remaining docs: {after:,}\")\n\n# -----------------------------\n# 2. Remove short documents\n# -----------------------------\nbefore = len(corpus)\n\ncorpus = corpus[corpus[\"text\"].str.len() > 200].reset_index(drop=True)\n\nafter = len(corpus)\nprint(f\"Removed short docs: {before - after:,}\")\nprint(f\"Remaining docs: {after:,}\")\n\n# -----------------------------\n# 3. Limit corpus size (Kaggle safe)\n# -----------------------------\nMAX_DOCS = 300_000   # safe size\n\nif len(corpus) > MAX_DOCS:\n    corpus = corpus.iloc[:MAX_DOCS].reset_index(drop=True)\n\nprint(f\"Final corpus size: {len(corpus):,} documents\")\n\n# -----------------------------\n# Cleanup\n# -----------------------------\ngc.collect()\n\nprint(\"\\nâœ… Corpus reduction complete\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:17:18.046632Z","iopub.execute_input":"2026-02-09T08:17:18.047258Z","iopub.status.idle":"2026-02-09T08:17:23.733846Z","shell.execute_reply.started":"2026-02-09T08:17:18.047229Z","shell.execute_reply":"2026-02-09T08:17:23.733126Z"}},"outputs":[{"name":"stdout","text":"Reducing corpus size safely...\n\nRemoved duplicate docs: 532,933\nRemaining docs: 2,119,315\nRemoved short docs: 301,918\nRemaining docs: 1,817,397\nFinal corpus size: 300,000 documents\n\nâœ… Corpus reduction complete\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# 5.1 - Create Chunks","metadata":{}},{"cell_type":"code","source":"print(\"Creating chunks...\\n\")\n\nCHUNK_SIZE = 800\nCHUNK_OVERLAP = 100\nstep = CHUNK_SIZE - CHUNK_OVERLAP\n\nchunk_records = []\n\nfor idx, text in tqdm(\n    zip(corpus.index, corpus[\"clean\"]),\n    total=len(corpus),\n    desc=\"Chunking\"\n):\n    tokens = text.split()\n    n = len(tokens)\n\n    # short document\n    if n <= CHUNK_SIZE:\n        chunk_records.append({\n            \"doc_idx\": idx,\n            \"text\": text\n        })\n        continue\n\n    # sliding window chunks\n    for start in range(0, n, step):\n        end = min(start + CHUNK_SIZE, n)\n\n        chunk_records.append({\n            \"doc_idx\": idx,\n            \"text\": \" \".join(tokens[start:end])\n        })\n\n        if end == n:\n            break\n\nchunks = pd.DataFrame(chunk_records)\n\nprint(\"\\nChunks created:\", len(chunks))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:17:30.784136Z","iopub.execute_input":"2026-02-09T08:17:30.784447Z","iopub.status.idle":"2026-02-09T08:17:34.453708Z","shell.execute_reply.started":"2026-02-09T08:17:30.784421Z","shell.execute_reply":"2026-02-09T08:17:34.452941Z"}},"outputs":[{"name":"stdout","text":"Creating chunks...\n\n","output_type":"stream"},{"name":"stderr","text":"Chunking: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300000/300000 [00:03<00:00, 86900.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nChunks created: 302101\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"print(\"Removing duplicate chunks...\")\n\nbefore_chunks = len(chunks)\n\nchunks = chunks.drop_duplicates(subset=[\"text\"]).reset_index(drop=True)\n\nafter_chunks = len(chunks)\n\nprint(f\"Removed {before_chunks - after_chunks:,} duplicate chunks\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:17:58.726689Z","iopub.execute_input":"2026-02-09T08:17:58.726989Z","iopub.status.idle":"2026-02-09T08:17:58.936765Z","shell.execute_reply.started":"2026-02-09T08:17:58.726964Z","shell.execute_reply":"2026-02-09T08:17:58.935988Z"}},"outputs":[{"name":"stdout","text":"Removing duplicate chunks...\nRemoved 244 duplicate chunks\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Cell 6: Build BM25 Indexes\n\nCreate BM25 indexes for sparse retrieval.","metadata":{}},{"cell_type":"code","source":"from rank_bm25 import BM25Okapi\n\nprint(\"Building BM25 index...\\n\")\n\n# tokenize chunks\nchunk_tokens = chunks[\"text\"].fillna(\"\").apply(\n    lambda x: x.split()[:200]\n).tolist()\n\nbm25_chunks = BM25Okapi(chunk_tokens)\n\ndel chunk_tokens\ngc.collect()\n\nprint(\"âœ… BM25 index ready\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:18:03.203499Z","iopub.execute_input":"2026-02-09T08:18:03.204295Z","iopub.status.idle":"2026-02-09T08:18:19.081208Z","shell.execute_reply.started":"2026-02-09T08:18:03.204265Z","shell.execute_reply":"2026-02-09T08:18:19.080488Z"}},"outputs":[{"name":"stdout","text":"Building BM25 index...\n\nâœ… BM25 index ready\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import numpy as np\n\ndef search_bm25_chunks(query, k=800):\n    tokens = clean(query).split()\n\n    scores = bm25_chunks.get_scores(tokens)\n\n    k = min(k, len(scores))\n    idx = np.argpartition(scores, -k)[-k:]\n    idx = idx[np.argsort(scores[idx])[::-1]]\n\n    return idx\n\nprint(\"âœ… BM25 search ready\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:18:26.657213Z","iopub.execute_input":"2026-02-09T08:18:26.657936Z","iopub.status.idle":"2026-02-09T08:18:26.662845Z","shell.execute_reply.started":"2026-02-09T08:18:26.657906Z","shell.execute_reply":"2026-02-09T08:18:26.662197Z"}},"outputs":[{"name":"stdout","text":"âœ… BM25 search ready\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Cell 7: Query Expansion (PRF)\n\nPseudo-Relevance Feedback for query expansion.","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\n# ------------------------------\n# German stopwords\n# ------------------------------\nSTOPWORDS = set(\n    \"der die das ein eine und oder aber wenn weil dass mit dem den auf aus \"\n    \"von in zu an bei fÃ¼r nach vor Ã¼ber unter zwischen durch sich es sie wir \"\n    \"ihr sein auch noch nur schon sehr wie was wo wer welch doch ja nein \"\n    \"nicht kein jede jeden jeder manch dieselbe dieselben diese diesen \"\n    \"dieser dem denen solch solche solchen solcher einem einen wurde \"\n    \"werden war waren sein ist sind\"\n    .split()\n)\n\n# ------------------------------\n# Query expansion\n# ------------------------------\ndef expand_query(query: str, n_docs: int = 3, n_terms: int = 8):\n    \"\"\"Pseudo relevance feedback expansion.\"\"\"\n\n    query_clean = clean(query)\n    original_tokens = set(query_clean.split())\n\n    # Retrieve top documents\n    top_docs = search_bm25_docs(query, k=n_docs)\n\n    term_freq = Counter()\n\n    for doc_idx in top_docs:\n        # Limit tokens processed per doc\n        tokens = corpus.iloc[doc_idx][\"clean\"].split()[:200]\n\n        for token in tokens:\n            if (\n                token in original_tokens\n                or token in STOPWORDS\n                or len(token) < 3\n            ):\n                continue\n\n            term_freq[token] += 1\n\n    # Add top expansion terms\n    extra_terms = [t for t, _ in term_freq.most_common(n_terms)]\n\n    expanded_query = query_clean + \" \" + \" \".join(extra_terms)\n\n    return expanded_query.strip()\n\nprint(\"âœ… Query expansion ready\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:18:32.382837Z","iopub.execute_input":"2026-02-09T08:18:32.383104Z","iopub.status.idle":"2026-02-09T08:18:32.390641Z","shell.execute_reply.started":"2026-02-09T08:18:32.383082Z","shell.execute_reply":"2026-02-09T08:18:32.389669Z"}},"outputs":[{"name":"stdout","text":"âœ… Query expansion ready\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## Cell 8: Load Embedding Model\n\nLoad multilingual E5-Large for dense retrieval.","metadata":{}},{"cell_type":"code","source":"print(\"Loading embedding model...\\n\")\n\nmodels_to_try = [\n    \"intfloat/multilingual-e5-small\",          # best Kaggle choice\n    \"BAAI/bge-small-en-v1.5\",                  # strong fallback\n    \"sentence-transformers/all-MiniLM-L6-v2\"   # safest fallback\n]\n\nembed_model = None\n\nfor model_name in models_to_try:\n    try:\n        print(f\"Trying: {model_name}\")\n        embed_model = SentenceTransformer(model_name, device=DEVICE)\n        print(f\"âœ… Loaded: {model_name}\")\n        break\n    except Exception as e:\n        print(f\"âš ï¸ Failed: {model_name} â†’ {e}\")\n\nif embed_model is None:\n    raise RuntimeError(\"No embedding model could be loaded.\")\n\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:18:36.8239Z","iopub.execute_input":"2026-02-09T08:18:36.824411Z","iopub.status.idle":"2026-02-09T08:18:43.856211Z","shell.execute_reply.started":"2026-02-09T08:18:36.824383Z","shell.execute_reply":"2026-02-09T08:18:43.855587Z"}},"outputs":[{"name":"stdout","text":"Loading embedding model...\n\nTrying: intfloat/multilingual-e5-small\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4916d561f3544433b0eb4f429681c6bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0185908683584071852c9b3bf9cc5556"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2decd5a9a5704e8ba64122c77f3c6706"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/655 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bfc31eb04a641e3a9b023dddb8b5793"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0a089083d0a4c70874015c9212ccb20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e38120a1dbea4c6682b3fbd42e29affe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1101f5397246418b9b3d5be85ff4fb45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0b3b117e61c4249b668be81303d2e8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/167 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"426376b7a7634334a2702803c69259ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c7cbe4b746d4d298be1b8ff9ee6b13d"}},"metadata":{}},{"name":"stdout","text":"âœ… Loaded: intfloat/multilingual-e5-small\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Cell 9: Encode Chunks & Build FAISS Index\n\nCreate embeddings and build FAISS index with caching.","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport torch\nimport gc\n\nprint(\"Loading embedding model...\\n\")\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nembed_model = SentenceTransformer(\n    \"intfloat/multilingual-e5-small\",\n    device=DEVICE\n)\n\nprint(\"âœ… Embedding model ready\")\n\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:18:49.919701Z","iopub.execute_input":"2026-02-09T08:18:49.920382Z","iopub.status.idle":"2026-02-09T08:18:53.925453Z","shell.execute_reply.started":"2026-02-09T08:18:49.920355Z","shell.execute_reply":"2026-02-09T08:18:53.914538Z"}},"outputs":[{"name":"stdout","text":"Loading embedding model...\n\nâœ… Embedding model ready\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"print(\"Generating embeddings...\\n\")\n\nEMBED_BATCH = 64\nchunk_batch_size = 20000\n\ntexts_series = chunks[\"text\"]\nall_embs = []\n\nfor i in range(0, len(texts_series), chunk_batch_size):\n    end = min(i + chunk_batch_size, len(texts_series))\n\n    batch_texts = [\n        \"passage: \" + t\n        for t in texts_series.iloc[i:end]\n    ]\n\n    batch_embs = embed_model.encode(\n        batch_texts,\n        batch_size=EMBED_BATCH,\n        normalize_embeddings=True,\n        convert_to_numpy=True,\n        show_progress_bar=True\n    ).astype(\"float32\")\n\n    all_embs.append(batch_embs)\n\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\nchunk_embeddings = np.vstack(all_embs)\n\nprint(\"âœ… Embeddings created:\", chunk_embeddings.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:18:59.25507Z","iopub.execute_input":"2026-02-09T08:18:59.260624Z","iopub.status.idle":"2026-02-09T08:42:26.802952Z","shell.execute_reply.started":"2026-02-09T08:18:59.260394Z","shell.execute_reply":"2026-02-09T08:42:26.802296Z"}},"outputs":[{"name":"stdout","text":"Generating embeddings...\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fcfc73db8354be98f1949e000508b85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a56352da5784f84a5eec49fd01ba185"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69a922a1427243158f1ba2c85c43af4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d052a3ea3ef4d048e6cfb0edda30b6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13fe0c0cf45541c996932a339964cc80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e345574511a4f64826db915eb2a104b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e568126405b04a538a4bf507f38b49a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0b764db8798400f982dfd58f115f3e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1225fb0fced4e0fb5937aa29f788d1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2117b2899acf4b48a4af34446d0a098a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91f68ecc984c4d0dac7157b7ce610a5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a98a8dd8b18a4e448ebb45f7f15c3169"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72311801b92240bdb146a7c0ebdd891d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"155de1ef6f8e4053b2611cc36562001e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4f26318d5f54cef9807d9455247beab"}},"metadata":{}},{"name":"stdout","text":"âœ… Embeddings created: (301857, 384)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import faiss\n\nprint(\"Building FAISS index...\\n\")\n\ndim = chunk_embeddings.shape[1]\n\nfaiss_index = faiss.IndexFlatIP(dim)\n\nfaiss_index.add(chunk_embeddings)\n\nprint(\"âœ… FAISS index ready\")\nprint(\"Total vectors:\", faiss_index.ntotal)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:46:48.620937Z","iopub.execute_input":"2026-02-09T08:46:48.621237Z","iopub.status.idle":"2026-02-09T08:46:49.008676Z","shell.execute_reply.started":"2026-02-09T08:46:48.62121Z","shell.execute_reply":"2026-02-09T08:46:49.008057Z"}},"outputs":[{"name":"stdout","text":"Building FAISS index...\n\nâœ… FAISS index ready\nTotal vectors: 301857\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# 9.1 - Dense Search Function\n\n","metadata":{}},{"cell_type":"code","source":"print(\"Preparing dense search...\\n\")\n\ndef search_dense(query, k=300):\n    q_emb = embed_model.encode(\n        [\"query: \" + clean(query)],\n        normalize_embeddings=True,\n        convert_to_numpy=True\n    ).astype(\"float32\")\n\n    scores, indices = faiss_index.search(q_emb, k)\n\n    return indices[0], scores[0]\n\nprint(\"âœ… Dense search ready\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:46:53.958982Z","iopub.execute_input":"2026-02-09T08:46:53.959279Z","iopub.status.idle":"2026-02-09T08:46:53.964008Z","shell.execute_reply.started":"2026-02-09T08:46:53.959254Z","shell.execute_reply":"2026-02-09T08:46:53.963216Z"}},"outputs":[{"name":"stdout","text":"Preparing dense search...\n\nâœ… Dense search ready\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## Cell 10: Load Cross-Encoder\n\nLoad cross-encoder for reranking.","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import CrossEncoder\n\nprint(\"Loading cross-encoder...\\n\")\n\nreranker = None\nHAS_RERANKER = False\n\ntry:\n    reranker = CrossEncoder(\n        \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n        max_length=512,\n        device=DEVICE\n    )\n    HAS_RERANKER = True\n    print(\"âœ… Cross-encoder loaded\")\n\nexcept Exception as e:\n    print(\"âš ï¸ Cross-encoder download failed.\")\n    print(\"Using retrieval without reranker.\")\n    HAS_RERANKER = False\n\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:46:58.583267Z","iopub.execute_input":"2026-02-09T08:46:58.583768Z","iopub.status.idle":"2026-02-09T08:47:03.197634Z","shell.execute_reply.started":"2026-02-09T08:46:58.583739Z","shell.execute_reply":"2026-02-09T08:47:03.196772Z"}},"outputs":[{"name":"stdout","text":"Loading cross-encoder...\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"092de1bc902f443faa7c4e4ae071482e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87b7a81d1bde479ba6a90308f8a9933c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc4b207b451a436a8fab8a9eb5f37b25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58917ac85ed849a89128a304dfa1405a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffed254e03374d2aa3199f5cf9dff49b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b86c1078e89249e8a11dccd6844983b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2078808e51a94e69894a31cb7427752f"}},"metadata":{}},{"name":"stdout","text":"âœ… Cross-encoder loaded\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Cell 11: Ranking Functions\n\nImplement RRF and reranking.","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\n\n# ------------------------------\n# RRF Fusion\n# ------------------------------\ndef fuse_rankings(bm25_ids, dense_ids, top_n: int = 300):\n    \"\"\"Reciprocal Rank Fusion.\"\"\"\n    scores = defaultdict(float)\n\n    for rank, cid in enumerate(bm25_ids):\n        scores[int(cid)] += 1.0 / (RRF_K + rank + 1)\n\n    for rank, cid in enumerate(dense_ids):\n        scores[int(cid)] += 1.0 / (RRF_K + rank + 1)\n\n    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n\n    return [cid for cid, _ in ranked[:top_n]]\n\n\n# ------------------------------\n# Chunk â†’ Document collapse\n# ------------------------------\ndef dedupe_to_docs(chunk_ids):\n    \"\"\"Collapse chunks to unique documents.\"\"\"\n    seen = set()\n    result = []\n\n    # Faster column access\n    doc_indices = chunks[\"doc_idx\"].values\n\n    for cid in chunk_ids:\n        doc_id = int(doc_indices[cid])\n\n        if doc_id not in seen:\n            seen.add(doc_id)\n            result.append(doc_id)\n\n    return result\n\n\n# ------------------------------\n# Cross-encoder reranking\n# ------------------------------\ndef rerank(query: str, doc_ids, top_k: int = 20):\n    \"\"\"Cross-encoder reranking.\"\"\"\n    if not HAS_RERANKER:\n        return doc_ids[:top_k]\n\n    # Limit docs passed to reranker\n    doc_ids = doc_ids[:50]\n\n    pairs = [\n        (query, corpus.iloc[did][\"text\"][:MAX_DOC_CHARS])\n        for did in doc_ids\n    ]\n\n    scores = reranker.predict(pairs, show_progress_bar=False)\n\n    best = np.argsort(scores)[::-1][:top_k]\n\n    return [doc_ids[i] for i in best]\n\nprint(\"âœ… Ranking functions ready\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:47:11.481876Z","iopub.execute_input":"2026-02-09T08:47:11.482143Z","iopub.status.idle":"2026-02-09T08:47:11.491337Z","shell.execute_reply.started":"2026-02-09T08:47:11.48212Z","shell.execute_reply":"2026-02-09T08:47:11.490648Z"}},"outputs":[{"name":"stdout","text":"âœ… Ranking functions ready\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## Cell 12: Main Retrieval Pipeline\n\n","metadata":{}},{"cell_type":"code","source":"print(\"Building retrieval pipeline...\\n\")\n\ndef retrieve(query, top_k=5):\n    \n    # --- Sparse search ---\n    bm25_ids = search_bm25_chunks(query, k=800)\n\n    # --- Dense search ---\n    dense_ids, _ = search_dense(query, k=300)\n\n    # Merge rankings\n    combined = list(dict.fromkeys(\n        list(bm25_ids) + list(dense_ids)\n    ))\n\n    # Convert chunk â†’ document\n    doc_ids = []\n    seen = set()\n\n    for cid in combined:\n        did = chunks.iloc[cid][\"doc_idx\"]\n\n        if did not in seen:\n            seen.add(did)\n            doc_ids.append(did)\n\n        if len(doc_ids) >= top_k:\n            break\n\n    # Return citations\n    return corpus.iloc[doc_ids][\"citation\"].tolist()\n\nprint(\"âœ… Retrieval ready\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:47:15.909706Z","iopub.execute_input":"2026-02-09T08:47:15.909972Z","iopub.status.idle":"2026-02-09T08:47:15.916072Z","shell.execute_reply.started":"2026-02-09T08:47:15.909951Z","shell.execute_reply":"2026-02-09T08:47:15.915405Z"}},"outputs":[{"name":"stdout","text":"Building retrieval pipeline...\n\nâœ… Retrieval ready\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## Cell 13: Evaluation (Optional)\n\nEvaluate quality if gold citations are available.","metadata":{}},{"cell_type":"code","source":"def retrieve(query, top_k=5):\n\n    bm25_ids = search_bm25_chunks(query, k=1200)\n    dense_ids, _ = search_dense(query, k=600)\n\n    combined = []\n    seen_chunks = set()\n\n    for b, d in zip(bm25_ids, dense_ids):\n        if b not in seen_chunks:\n            combined.append(b)\n            seen_chunks.add(b)\n        if d not in seen_chunks:\n            combined.append(d)\n            seen_chunks.add(d)\n\n    docs = []\n    seen_docs = set()\n\n    for cid in combined:\n        did = chunks.iloc[cid][\"doc_idx\"]\n\n        if did not in seen_docs:\n            seen_docs.add(did)\n            docs.append(did)\n\n        if len(docs) >= top_k:\n            break\n\n    return corpus.iloc[docs][\"citation\"].tolist()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:47:27.041721Z","iopub.execute_input":"2026-02-09T08:47:27.042446Z","iopub.status.idle":"2026-02-09T08:47:27.048014Z","shell.execute_reply.started":"2026-02-09T08:47:27.042419Z","shell.execute_reply":"2026-02-09T08:47:27.047297Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"\nquery = \"your legal question here\"\n\nresults = retrieve(query, top_k=5)\n\nprint(\"Query:\", query)\nprint(\"Results:\")\nfor r in results:\n    print(\"-\", r)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:47:30.977766Z","iopub.execute_input":"2026-02-09T08:47:30.978364Z","iopub.status.idle":"2026-02-09T08:47:31.542639Z","shell.execute_reply.started":"2026-02-09T08:47:30.978335Z","shell.execute_reply":"2026-02-09T08:47:31.541843Z"}},"outputs":[{"name":"stdout","text":"Query: your legal question here\nResults:\n- 4A_80/2013 E. A\n- 5A_878/2012 E. 1.2.2\n- BGE 147 III 586 E. 4.4.2\n- 4A_146/2013 E. 2.1\n- BGE 143 II 350 E. 4.1\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"## Cell 14: Generate Predictions\n\nRun the pipeline on all queries and generate submission file.","metadata":{}},{"cell_type":"code","source":"print(\"Generating submission (final fast mode)...\")\n\n# Load queries\ntest_queries = pd.read_csv(\n    \"/kaggle/input/llm-agentic-legal-information-retrieval/test.csv\"\n)\n\nqueries_list = test_queries[\"query\"].fillna(\"\").tolist()\n\n# Batch encode queries\nquery_embeddings = embed_model.encode(\n    [\"query: \" + q for q in queries_list],\n    batch_size=128,\n    normalize_embeddings=True,\n    convert_to_numpy=True,\n    show_progress_bar=False\n).astype(\"float32\")\n\n# Fast lookup arrays\nchunk_to_doc = chunks[\"doc_idx\"].values\ncitations = corpus[\"citation\"].values\n\npredictions = []\n\nfor q_emb in query_embeddings:\n\n    _, dense_ids = faiss_index.search(\n        q_emb.reshape(1, -1), 600\n    )\n\n    docs, seen = [], set()\n\n    for cid in dense_ids[0]:\n        did = chunk_to_doc[cid]\n\n        if did not in seen:\n            seen.add(did)\n            docs.append(citations[did])\n\n        if len(docs) >= 5:\n            break\n\n    predictions.append(\";\".join(docs))\n\nsubmission = pd.DataFrame({\n    \"query_id\": test_queries[\"query_id\"],\n    \"gold_citations\": predictions\n})\n\nsubmission.to_csv(\"/kaggle/working/submission.csv\", index=False)\n\nprint(\"âœ… Submission saved\")\nprint(\"Rows:\", len(submission))\nprint(submission.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:47:40.999195Z","iopub.execute_input":"2026-02-09T08:47:40.999514Z","iopub.status.idle":"2026-02-09T08:47:43.016411Z","shell.execute_reply.started":"2026-02-09T08:47:40.999486Z","shell.execute_reply":"2026-02-09T08:47:43.01577Z"}},"outputs":[{"name":"stdout","text":"Generating submission (final fast mode)...\nâœ… Submission saved\nRows: 40\n   query_id                                     gold_citations\n0  test_001  BGE 141 I 172 E. 5.3.5;BGE 146 IV 36 E. 2.3;6B...\n1  test_002  9C_371/2015 E. 4.3.2;9C_899/2015 E. 3.1;8C_415...\n2  test_003  6B_718/2018 E. 4.3.3;1B_41/2016 E. 1.2;4A_544/...\n3  test_004  5A_46/2022 E. 3;BGE 150 III 315 E. 6.3.3;5A_89...\n4  test_005  4A_478/2015 E. 3.3.1;4A_287/2016 E. 1.1;4A_173...\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"## Cell 15: Summary\n\n","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\" * 60)\nprint(\"LEGAL CITATION RETRIEVAL - PIPELINE COMPLETE\")\nprint(\"=\" * 60)\n\nprint(\"\\nðŸ“Š Final Summary:\")\n\nif \"corpus\" in globals():\n    print(f\"  Corpus: {len(corpus):,} documents\")\n\nif \"chunks\" in globals():\n    print(f\"  Chunks: {len(chunks):,} chunks\")\n\nif \"queries\" in globals():\n    print(f\"  Queries: {len(queries):,} questions\")\n\nif \"output\" in globals():\n    print(f\"  Predictions: {len(output):,} results\")\n\nprint(\"\\nðŸ’¾ Output:\")\nprint(\"  File: submission.csv\")\nprint(\"  Location: /kaggle/working/\")\nprint(\"  Columns: query_id, gold_citations\")\n\nprint(\"\\nâœ… Pipeline Complete!\")\nprint(\"\\nðŸ“¥ Download submission.csv from /kaggle/working/\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T08:47:56.295965Z","iopub.execute_input":"2026-02-09T08:47:56.296538Z","iopub.status.idle":"2026-02-09T08:47:56.302423Z","shell.execute_reply.started":"2026-02-09T08:47:56.29649Z","shell.execute_reply":"2026-02-09T08:47:56.301703Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nLEGAL CITATION RETRIEVAL - PIPELINE COMPLETE\n============================================================\n\nðŸ“Š Final Summary:\n  Corpus: 300,000 documents\n  Chunks: 301,857 chunks\n\nðŸ’¾ Output:\n  File: submission.csv\n  Location: /kaggle/working/\n  Columns: query_id, gold_citations\n\nâœ… Pipeline Complete!\n\nðŸ“¥ Download submission.csv from /kaggle/working/\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}